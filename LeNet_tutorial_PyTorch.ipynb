{"nbformat_minor": 1, "cells": [{"source": "!conda install pytorch torchvision -c pytorch", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Fetching package metadata .............\nSolving package specifications: .\n\nPackage plan for installation in environment /opt/conda/envs/DSX-Python35:\n\nThe following packages will be UPDATED:\n\n    torchvision: 0.2.1-py35_0 --> 0.2.1-py35_1 pytorch\n\n"}], "execution_count": 7}, {"source": "#https://github.com/pytorch/examples/blob/master/mnist/main.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport time as time", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 8}, {"source": "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        \n        # input is 28x28x1\n        # conv1(kernel=5, filters=10) 28x28x10 -> 24x24x10\n        # max_pool(kernel=2) 24x24x10 -> 12x12x10\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        \n        # conv2(kernel=5, filters=20) 12x12x20 -> 8x8x20\n        # max_pool(kernel=2) 8x8x20 -> 4x4x20\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        \n        # flatten 4x4x20 = 320\n        x = x.view(-1, 320)\n        # 320 -> 50\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        # 50 -> 10\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 9}, {"source": "def train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 100 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 10}, {"source": "def main():\n    # Training settings\n    batch_size = 128\n    test_batch_size = 1000\n    epochs = 2\n    lr = 0.05\n    momentum = 0.5\n    seed = 42\n    \n    use_cuda = False\n    torch.manual_seed(seed)\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n      \n    # download and transform train dataset\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('../data', train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])), batch_size = batch_size, shuffle=True, num_workers=4)\n    \n    # download and transform test dataset\n    test_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=False, transform=transforms.Compose([\n                           transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))\n                       ])), batch_size = test_batch_size, shuffle=True, num_workers=4)\n\n\n    model = Net().to(device)\n    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n\n    for epoch in range(1, epochs + 1):\n        start = time.time()\n        train(model, device, train_loader, optimizer, epoch)\n        print('Training took {:.3f} s'.format((time.time()-start)))\n        test(model, device, test_loader)\n\n\nif __name__ == '__main__':\n    main()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.307264\nTrain Epoch: 1 [12800/60000 (21%)]\tLoss: 0.648696\nTrain Epoch: 1 [25600/60000 (43%)]\tLoss: 0.526176\nTrain Epoch: 1 [38400/60000 (64%)]\tLoss: 0.470979\nTrain Epoch: 1 [51200/60000 (85%)]\tLoss: 0.291871\nTraining took 144.115 s\n\nTest set: Average loss: 0.1250, Accuracy: 9621/10000 (96%)\n\nTrain Epoch: 2 [0/60000 (0%)]\tLoss: 0.235891\nTrain Epoch: 2 [12800/60000 (21%)]\tLoss: 0.316604\nTrain Epoch: 2 [25600/60000 (43%)]\tLoss: 0.331153\nTrain Epoch: 2 [38400/60000 (64%)]\tLoss: 0.374789\nTrain Epoch: 2 [51200/60000 (85%)]\tLoss: 0.242344\nTraining took 145.230 s\n\nTest set: Average loss: 0.0815, Accuracy: 9756/10000 (98%)\n\n"}], "execution_count": 19}, {"source": "", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3.5", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.5", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}